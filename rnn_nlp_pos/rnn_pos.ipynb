{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    words = line.split()\n",
    "    tokens,tags = [],[]\n",
    "    for word in words:\n",
    "        try:\n",
    "            params = word.split(\"_\")\n",
    "            tokens.append(params[0])\n",
    "            tags.append(params[1])\n",
    "        except IndexError as e:\n",
    "            e\n",
    "            #dirtydata\n",
    "            #print(\"index error, skip data\")\n",
    "    return (tokens,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"victorian_BRAND order_BRAND of_BRAND nurses_BRAND lemarchant_STREET rd_STREET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokens,tags) = parseLine(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['victorian', 'order', 'of', 'nurses', 'lemarchant', 'rd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRAND', 'BRAND', 'BRAND', 'BRAND', 'STREET', 'STREET']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,sequences_tags = [],[]\n",
    "fh = open('can_test.txt')\n",
    "for line in fh:\n",
    "    tokens,tags = parseLine(line)\n",
    "    if(len(tokens) != 0):\n",
    "        sequences.append(np.array(tokens))\n",
    "        sequences_tags.append(np.array(tags))\n",
    "fh.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nancy' 'ave']\n",
      "['STREET' 'STREET']\n"
     ]
    }
   ],
   "source": [
    "print(sequences[5])\n",
    "print(sequences_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sequences, \n",
    " test_sequences, \n",
    " train_tags, \n",
    " test_tags) = train_test_split(sequences, sequences_tags, train_size=0.8,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    " \n",
    "# \n",
    "#(train_sentences, \n",
    "# test_sentences, \n",
    "## test_tags) = train_test_split(sequences, sequences_tags, test_size=0.2)\n",
    "#train_test_spli?\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sequences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "        \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 2]\n",
      "[6, 2, 7]\n",
      "[2, 1, 1]\n",
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "train_sequences_X, test_sequences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sequences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sequences_X.append(s_int)\n",
    "\n",
    "for s in test_sequences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sequences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "        try:\n",
    "            train_tags_y.append([tag2index[t] for t in s])\n",
    "        except:\n",
    "            train_tags_y.append([0])\n",
    "\n",
    "for s in test_tags:\n",
    "        try:\n",
    "            test_tags_y.append([tag2index[t] for t in s])\n",
    "        except:\n",
    "            test_tags_y.append([0])\n",
    "        \n",
    "print(train_sequences_X[0])\n",
    "print(test_sequences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "3\n",
      "10\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sequences_X))\n",
    "print(len(test_sequences_X))\n",
    "\n",
    "print(len(train_tags_y))\n",
    "print(len(test_tags_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sequences_X, key=len))\n",
    "print(MAX_LENGTH)  # 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 6 2]\n",
      "[6 2 7]\n",
      "[2 1 1]\n",
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sequences_X = pad_sequences(train_sequences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sequences_X = pad_sequences(test_sequences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sequences_X[0])\n",
    "print(test_sequences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 128)            1408      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 3, 512)            788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 3, 3)              1539      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3, 3)              0         \n",
      "=================================================================\n",
      "Total params: 791,427\n",
      "Trainable params: 791,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 8 samples, validate on 2 samples\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 1.1007 - acc: 0.1250 - val_loss: 1.0846 - val_acc: 0.6667\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0857 - acc: 0.6667 - val_loss: 1.0712 - val_acc: 0.6667\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.0706 - acc: 0.6667 - val_loss: 1.0572 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_sequences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=1024, epochs=3, validation_split=0.2)\n",
    "model.save(filepath=\"rnn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 654us/step\n",
      "acc: 66.66666865348816\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sequences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags_y = to_categorical(test_tags_y, len(tag2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_tags_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_sequences_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['king', 'street'], ['wayfare', 'restauran', 'main', 'st', '311'], ['falls', 'cadillac', 'pres', 'de', 'la', 'thorold', 'stone', 'rd']]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"king street\".split(),\n",
    "    \"wayfare restauran main st 311\".split(),\n",
    "    \"falls cadillac pres de la thorold stone rd\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 52117  64495      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [     1      1  23203   3147  92094      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.44228033e-07 1.85927883e-01 1.87560431e-06 6.38416503e-04\n",
      "   8.13339353e-01 9.22444888e-05]\n",
      "  [1.28117623e-04 4.80535746e-01 2.49975960e-06 7.93299172e-04\n",
      "   5.18474996e-01 6.53061506e-05]\n",
      "  [9.98677790e-01 1.28250110e-06 8.40640908e-08 1.13478210e-03\n",
      "   1.85615994e-04 3.11253928e-07]\n",
      "  [9.99999642e-01 1.70752071e-13 2.87434833e-11 2.49423550e-07\n",
      "   1.22943689e-07 9.34411437e-11]\n",
      "  [1.00000000e+00 5.36143963e-17 5.15426731e-13 8.86929641e-09\n",
      "   1.81319426e-09 1.59441752e-12]\n",
      "  [1.00000000e+00 4.20488860e-17 9.67074185e-13 1.14477476e-08\n",
      "   3.41589157e-09 2.77487235e-12]\n",
      "  [1.00000000e+00 8.99147179e-17 3.41162818e-12 2.65833755e-08\n",
      "   1.16623173e-08 7.38388257e-12]\n",
      "  [1.00000000e+00 1.12615823e-16 6.96862056e-12 4.48944775e-08\n",
      "   2.28640857e-08 9.85467871e-12]\n",
      "  [1.00000000e+00 7.99053295e-17 7.98133781e-12 4.86602332e-08\n",
      "   2.51160746e-08 6.94465752e-12]\n",
      "  [1.00000000e+00 4.07320178e-17 6.28681655e-12 3.52419782e-08\n",
      "   1.85692333e-08 3.48387009e-12]\n",
      "  [1.00000000e+00 1.92867384e-17 4.31617459e-12 1.99430179e-08\n",
      "   1.09816964e-08 1.67201279e-12]\n",
      "  [1.00000000e+00 9.88812531e-18 3.16126422e-12 1.08604450e-08\n",
      "   6.05293948e-09 9.35573559e-13]\n",
      "  [1.00000000e+00 6.15365135e-18 2.94458273e-12 6.74153888e-09\n",
      "   3.54943919e-09 7.15582758e-13]\n",
      "  [1.00000000e+00 5.28018833e-18 4.14395818e-12 5.52113288e-09\n",
      "   2.52133536e-09 8.74567346e-13]\n",
      "  [1.00000000e+00 7.00371095e-18 9.94415834e-12 6.70173117e-09\n",
      "   2.47420062e-09 1.88306684e-12]\n",
      "  [1.00000000e+00 1.52532254e-17 4.03024662e-11 1.27674999e-08\n",
      "   3.61937658e-09 7.02125380e-12]\n",
      "  [1.00000000e+00 5.52014151e-17 2.40236137e-10 3.72627973e-08\n",
      "   8.10203016e-09 4.00343161e-11]\n",
      "  [9.99999881e-01 3.43647699e-16 1.79862047e-09 1.52195909e-07\n",
      "   2.86513036e-08 3.07003062e-10]]\n",
      "\n",
      " [[1.24556630e-06 9.76930261e-01 2.06024251e-05 1.41579565e-02\n",
      "   7.74129899e-03 1.14862621e-03]\n",
      "  [3.68499855e-06 7.84260750e-01 6.59360157e-05 2.82550137e-02\n",
      "   1.81530580e-01 5.88418264e-03]\n",
      "  [5.15948784e-07 4.27751290e-03 1.57321676e-06 1.47210840e-05\n",
      "   9.95171249e-01 5.34444989e-04]\n",
      "  [1.46078918e-07 2.67442811e-04 2.84263528e-08 3.15744622e-08\n",
      "   9.99678373e-01 5.39907087e-05]\n",
      "  [3.45696439e-03 3.53687537e-05 7.08531275e-07 9.94174540e-01\n",
      "   2.31083389e-03 2.17101369e-05]\n",
      "  [9.99774277e-01 2.32246000e-09 2.82799757e-08 2.00900322e-04\n",
      "   2.46131185e-05 1.71648068e-07]\n",
      "  [9.99992967e-01 1.37203866e-12 8.72954320e-10 4.93430980e-06\n",
      "   2.15972273e-06 3.64142627e-09]\n",
      "  [9.99997377e-01 2.39318504e-14 1.34250402e-10 2.33124229e-06\n",
      "   2.71411778e-07 2.72856154e-10]\n",
      "  [9.99998093e-01 7.73151854e-15 8.07374514e-11 1.78517973e-06\n",
      "   1.30894250e-07 8.60410077e-11]\n",
      "  [9.99998689e-01 4.10746520e-15 5.62472291e-11 1.14897989e-06\n",
      "   7.44938404e-08 3.73090621e-11]\n",
      "  [9.99999404e-01 2.21895125e-15 3.82695368e-11 5.84119903e-07\n",
      "   4.15837462e-08 1.75205371e-11]\n",
      "  [9.99999762e-01 1.16375635e-15 2.76379111e-11 2.79056792e-07\n",
      "   2.38403164e-08 9.39916547e-12]\n",
      "  [9.99999881e-01 6.55688363e-16 2.44523256e-11 1.49901780e-07\n",
      "   1.47994594e-08 6.63513645e-12]\n",
      "  [9.99999881e-01 4.73710852e-16 3.18387261e-11 1.06515870e-07\n",
      "   1.10127605e-08 7.30384503e-12]\n",
      "  [9.99999881e-01 5.14993491e-16 6.97271893e-11 1.13084155e-07\n",
      "   1.10838387e-08 1.40681147e-11]\n",
      "  [9.99999762e-01 9.24615765e-16 2.57123406e-10 1.90211651e-07\n",
      "   1.61699791e-08 4.72311808e-11]\n",
      "  [9.99999523e-01 2.82785888e-15 1.40196132e-09 4.95566951e-07\n",
      "   3.52665701e-08 2.45680809e-10]\n",
      "  [9.99998093e-01 1.52341467e-14 9.68336789e-09 1.82647034e-06\n",
      "   1.19836841e-07 1.73607262e-09]]\n",
      "\n",
      " [[8.71309069e-08 9.98789132e-01 5.68851010e-06 9.70329565e-05\n",
      "   3.40761326e-05 1.07398781e-03]\n",
      "  [4.74251510e-06 9.47721064e-01 1.13525391e-04 2.95669097e-03\n",
      "   5.75700426e-04 4.86282147e-02]\n",
      "  [6.63077253e-06 2.10171402e-01 2.47751916e-04 1.60408113e-03\n",
      "   9.24304128e-03 7.78727114e-01]\n",
      "  [3.51358312e-06 3.08199860e-02 1.37843701e-04 1.33153633e-03\n",
      "   7.85454828e-03 9.59852517e-01]\n",
      "  [1.27607620e-06 2.28213854e-02 1.51121159e-04 2.41339915e-02\n",
      "   3.52901183e-02 9.17602062e-01]\n",
      "  [1.15272042e-05 1.02535747e-02 1.58587602e-04 1.06540643e-01\n",
      "   6.65693462e-01 2.17342123e-01]\n",
      "  [1.15068224e-06 3.42546526e-04 1.01164380e-06 1.13791803e-05\n",
      "   9.98511493e-01 1.13231933e-03]\n",
      "  [1.36516319e-04 2.42789174e-04 9.94973789e-07 1.40098780e-06\n",
      "   9.99017715e-01 6.00620231e-04]\n",
      "  [9.85628605e-01 1.62809329e-06 4.43131785e-06 1.38142472e-02\n",
      "   4.99466725e-04 5.16130822e-05]\n",
      "  [9.99957442e-01 3.74229231e-10 3.97324982e-08 3.00551892e-05\n",
      "   1.23430618e-05 1.46834353e-07]\n",
      "  [9.99997616e-01 1.06343774e-13 4.35766118e-10 2.20460129e-06\n",
      "   2.80806120e-07 6.11844575e-10]\n",
      "  [9.99999523e-01 3.13242377e-15 5.48020691e-11 4.72840384e-07\n",
      "   3.70600652e-08 3.50628242e-11]\n",
      "  [9.99999762e-01 8.92647260e-16 3.01441823e-11 2.20821988e-07\n",
      "   1.49608343e-08 1.23840608e-11]\n",
      "  [9.99999881e-01 5.76488189e-16 3.47358704e-11 1.51682514e-07\n",
      "   9.71101866e-09 1.09963288e-11]\n",
      "  [9.99999881e-01 6.27409321e-16 7.27293156e-11 1.57700057e-07\n",
      "   9.23225318e-09 1.94301311e-11]\n",
      "  [9.99999762e-01 1.15406143e-15 2.61977245e-10 2.62823477e-07\n",
      "   1.31102169e-08 6.24504823e-11]\n",
      "  [9.99999285e-01 3.63684988e-15 1.41131373e-09 6.87432873e-07\n",
      "   2.82534511e-08 3.17444321e-10]\n",
      "  [9.99997258e-01 2.02254138e-14 9.71308634e-09 2.56971293e-06\n",
      "   9.57275930e-08 2.21832686e-09]]] (3, 18, 6)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['STREET', 'STREET', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['BRAND', 'BRAND', 'STREET', 'STREET', 'DOOR', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['BRAND', 'BRAND', 'CONJ', 'CONJ', 'CONJ', 'STREET', 'STREET', 'STREET', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"onebox_tagging.rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagInput(input):\n",
    "    tokens = input.split()\n",
    "    test_samples_Xx = []\n",
    "    s_intx = []\n",
    "  \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            s_intx.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_intx.append(word2index['-OOV-'])\n",
    "        test_samples_Xx.append(s_intx)\n",
    "    print(\"before test samples \"+str(test_samples_Xx))\n",
    "    test_samples_Xx = pad_sequences(test_samples_Xx, maxlen=MAX_LENGTH, padding='post')\n",
    "    print(\"test samples \"+str(test_samples_Xx))\n",
    "    predictionsx = model.predict(test_samples_Xx)\n",
    "    tags = logits_to_tokens(predictionsx, {i: t for t, i in tag2index.items()})\n",
    "   \n",
    "    return tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before test samples [[100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718], [100704, 62859, 99304, 60819, 39226, 105553, 71431, 98718]]\n",
      "test samples [[100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]\n",
      " [100704  62859  99304  60819  39226 105553  71431  98718      0      0\n",
      "       0      0      0      0      0      0      0      0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-'],\n",
       " ['BRAND',\n",
       "  'BRAND',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'CONJ',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  'STREET',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-',\n",
       "  '-PAD-']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagInput(input = \"falls cadillac pres de la thorold stone rd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
